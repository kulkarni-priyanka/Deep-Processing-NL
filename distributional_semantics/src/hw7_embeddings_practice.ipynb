{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Word Embeddings\n",
    "\n",
    "In this exercise, we'll load a set of pre-trained word embeddings created with `gensim` and use them to explore similarity.\n",
    "\n",
    "Let's start by importing the `gensim` module and loading our pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_embeddings = gensim.models.Word2Vec.load('wsj-embeddings.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We can do a few operations on the embeddings, such as getting the number of words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44377\n"
     ]
    }
   ],
   "source": [
    "print(len(w2v_embeddings.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embeddings for Similarity\n",
    "\n",
    "More importantly, however, particularly for the purposes of the homework, we want to use our word embeddings to calculate similarities between words.\n",
    "\n",
    "Using the `gensim.models.Word2Vec` class, we can simply call:\n",
    "\n",
    "    w2v_embeddings.wv.simiarity(word_1, word_2)\n",
    "    \n",
    "To get the cosine similarity score between the vectors ([documentation here](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man                 woman           = 0.9413719773292542\n",
      "person              child           = 0.8452761173248291\n",
      "tax                 money           = 0.6546809673309326\n",
      "tax                 tariff          = 0.6185605525970459\n",
      "tax                 savings         = 0.47217410802841187\n",
      "tax                 dogs            = 0.43527430295944214\n",
      "pope                catholic        = 0.8322218656539917\n",
      "pope                senator         = 0.8930568695068359\n",
      "pope                leader          = 0.8474855422973633\n",
      "pope                tax             = 0.1644563525915146\n"
     ]
    }
   ],
   "source": [
    "def print_similarity(word_1, word_2):\n",
    "    \n",
    "    def check_word(word):\n",
    "        contained = word in w2v_embeddings.wv\n",
    "        if not contained:\n",
    "            print('\"{}\" not seen in embeddings.'.format(word))\n",
    "        return contained\n",
    "        \n",
    "    if check_word(word_1) and check_word(word_2):\n",
    "        sim = w2v_embeddings.wv.similarity(word_1, word_2)\n",
    "        print('{:<18}  {:<15} = {}'.format(word_1, word_2, sim))\n",
    "\n",
    "print_similarity('man', 'woman')\n",
    "print_similarity('person', 'child')\n",
    "print_similarity('tax', 'money')\n",
    "print_similarity('tax', 'tariff')\n",
    "print_similarity('tax', 'savings')\n",
    "print_similarity('tax', 'dogs')\n",
    "print_similarity('pope', 'catholic')\n",
    "print_similarity('pope', 'senator')\n",
    "print_similarity('pope', 'leader')\n",
    "print_similarity('pope', 'tax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlating with Human Judgments\n",
    "\n",
    "Now, I've designed a short in-class poll for us to go through a number of word pairs and get judgments from the class. We can use these results as a convenience sample of human judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "human_judgments = '''man,woman,5.0\n",
    "person,child,5.0\n",
    "tax,tariff,5.0\n",
    "tax,money,4.0\n",
    "tax,savings,3.0\n",
    "tax,dogs,1.0\n",
    "pope,catholic,4.0\n",
    "pope,senator,3.0\n",
    "pope,leader,3.0\n",
    "pope,tax,1.0\n",
    "'''\n",
    "\n",
    "import csv\n",
    "judgments = [(row[0], row[1], float(row[2])) for row in csv.reader(human_judgments.split('\\n')) if row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our human judgments obtained, let's use the human scores as array $a$ and the embeddings scores as array $b$, and use [`scipy.stats.spearmanr`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) to calculate the Spearman rank-order correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5315095895586142\n"
     ]
    }
   ],
   "source": [
    "human_scores = [entry[2] for entry in judgments]\n",
    "embedding_scores = [w2v_embeddings.wv.similarity(w1, w2) for w1, w2, score in judgments]\n",
    "\n",
    "from scipy.stats.stats import spearmanr\n",
    "\n",
    "print(spearmanr(human_scores, embedding_scores).correlation)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}