{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Similarity Practice\n",
    "\n",
    "This practice sheet should help you gain a little familiarity with creating a distributional representation for a word, and how to query it later.\n",
    "\n",
    "## 1. Getting our Data\n",
    "\n",
    "Our first step in creating a distributional representation for our vocabulary is to get our data set. This should be pretty familiar by now; find a resource with many sentences, and tokenize those sentences.\n",
    "\n",
    "*(NOTE: You may need to first launch a python interpreter and run the following:)*\n",
    "\n",
    "    >>> import nltk\n",
    "    >>> nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "brown_sents = nltk.corpus.brown.sents()\n",
    "print(brown_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Designing Our Data Store\n",
    "\n",
    "Now that we have the data that we'll be using for an input, we need to figure out the best way to store this data.\n",
    "\n",
    "Our target structure will let us look up a pair of words, $\\langle w_1, w_2 \\rangle$ and see how many times $w_2$ occurred within some window $n$ of $w_1$.\n",
    "\n",
    "A common, efficient way to store such information is to create a 2-dimensional matrix, where each row index is correlated to a unique vocabulary word which will be $w_1$, and every column index will represent $w_2$.\n",
    "\n",
    "Let's go ahead and make a data structure that allows us to easily increment counts of collocations, as well as allow us to look up a unique ID for a word, whether we've seen it, or wheter it's a new vocab item. We can do this with a standard matrix, or with nested dictionaries, if we prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class CollocationMatrix(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._word_mapping = {}  # Where we'll store string->int mapping.        \n",
    "    \n",
    "    def word_id(self, word, store_new=False):\n",
    "        \"\"\"\n",
    "        Return the integer ID for the given vocab item. If we haven't\n",
    "        seen this vocab item before, give ia a new ID. We can do this just\n",
    "        as 1, 2, 3... based on how many words we've seen before.\n",
    "        \"\"\"\n",
    "        if word not in self._word_mapping:\n",
    "            if store_new:\n",
    "                self._word_mapping[word] = len(self._word_mapping)\n",
    "                self[self._word_mapping[word]] = defaultdict(int)  # Also add a new row for this new word.\n",
    "            else:\n",
    "                return None\n",
    "        return self._word_mapping[word]\n",
    "    \n",
    "    def add_pair(self, w_1, w_2):\n",
    "        \"\"\"\n",
    "        Add a pair of colocated words into the coocurrence matrix.\n",
    "        \"\"\"\n",
    "        w_id_1 = self.word_id(w_1, store_new=True)\n",
    "        w_id_2 = self.word_id(w_2, store_new=True)\n",
    "        self[w_id_1][w_id_2] += 1  # Increment the count for this collocation\n",
    "        \n",
    "    def get_pair(self, w_1, w_2):\n",
    "        \"\"\"\n",
    "        Return the colocation for w_1, w_2\n",
    "        \"\"\"\n",
    "        w_1_id = self.word_id(w_1)\n",
    "        w_2_id = self.word_id(w_2)\n",
    "        if w_1_id and w_2_id:\n",
    "            return self[w_1_id][w_2_id]\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def get_row(self, word):\n",
    "        word_id = self.word_id(word)\n",
    "        if word_id is not None:\n",
    "            return self.get(word_id)\n",
    "        else:\n",
    "            return defaultdict(int)\n",
    "    \n",
    "    def get_row_sum(self, word):\n",
    "        \"\"\"\n",
    "        Get the number of total contexts available for a given word        \n",
    "        \"\"\"\n",
    "        return sum(self.get_row(word).values())\n",
    "    \n",
    "    def get_col_sum(self, word):\n",
    "        \"\"\"\n",
    "        Get the number of total contexts a given word occurs in\n",
    "        \"\"\"\n",
    "        f_id = self.word_id(word)\n",
    "        return sum([self[w][f_id] for w in self.keys()])\n",
    "    \n",
    "    @property\n",
    "    def total_sum(self):\n",
    "        return sum([self.get_row_sum(w) for w in self._word_mapping.keys()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Populating Our Colocation Data\n",
    "\n",
    "Now that we've got the data to store our colocations in, we need to populate it!\n",
    "\n",
    "This simple code steps through our sentences up to `sent_limit` using a window size of `window_size` and grabs the words within that window to add to to colocation matrix.\n",
    "\n",
    "Note that no special treatment is made for word-initial or word-final tokens here, but it's possible to create such a modification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "['Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'investigation', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'evidence', \"''\", 'irregularities', 'took', 'place']\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "['jury', 'said', 'term-end', 'presentments', 'City', 'Executive', 'Committee', 'over-all', 'charge', 'election', '``', 'deserves', 'praise', 'thanks', 'City', 'Atlanta', \"''\", 'manner', 'election', 'conducted']\n",
      "\"the\" has 0 contexts in the data.\n",
      "\"jury\" and \"grand\" seen together 1 times.\n",
      "\"primary\" and \"election\" seen together 1 times.\n",
      "\"midterm\" and \"election\" seen together 0 times.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "window_size = 3\n",
    "sent_limit = 2\n",
    "matrix = CollocationMatrix()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "for sent in brown_sents[:sent_limit]:\n",
    "    \n",
    "    print(sent)\n",
    "       \n",
    "    sent = [w for w in sent if w.lower() not in stopwords]\n",
    "    \n",
    "    sent = [w for w in sent if w.lower() not in string.punctuation]\n",
    "    \n",
    "    print(sent)\n",
    "        \n",
    "    for i, word in enumerate(sent):\n",
    "        # Increment the count of words we've seen.\n",
    "        for j in range(-window_size, window_size+1):\n",
    "            # Skip counting the word itself.\n",
    "            if j == 0:\n",
    "                continue\n",
    "                \n",
    "            # At the beginning and end of the sentence,\n",
    "            # you can either skip counting, or add a\n",
    "            # unique \"<START>\" or \"<END>\" token to indicate\n",
    "            # the word being colocated at the beginning or\n",
    "            # end of sentences.            \n",
    "            if len(sent) > i+j > 0:\n",
    "                word_1 = sent[i].lower()\n",
    "                word_2 = sent[i+j].lower()\n",
    "                \n",
    "                matrix.add_pair(word_1, word_2)                \n",
    "\n",
    "def print_colocate(w_1, w_2):\n",
    "    print('\"{}\" and \"{}\" seen together {} times.'.format(w_1, w_2,\n",
    "                                                     matrix.get_pair(w_1, w_2)))\n",
    "    \n",
    "def print_count(word):\n",
    "    print('\"{}\" has {} contexts in the data.'.format(word, \n",
    "                                                     matrix.get_row_sum(word)))\n",
    "\n",
    "print_count('the')                \n",
    "print_colocate('jury', 'grand')\n",
    "print_colocate('primary', 'election')\n",
    "print_colocate('midterm', 'election')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculating PMI\n",
    "\n",
    "Now, having collocation counts is handy, as we've discussed in class, but recall that \"the\" is going to collocate with all sorts of words, and so isn't all that helpful.\n",
    "\n",
    "Instead, we should try calculating Pointwise Mutual Information (PMI), which tells us, in essence, how likely it is to see two things in the same place, compared to seeing them independently.\n",
    "\n",
    "The formula is: $\\log \\frac{p(w,f)}{p(w)\\cdot p(f)}$, where $p(w,f)$ is the probability of seeing context $f$ for word $w$ out of all possible contexts; $p(w)$, is the probability of seeing word $w$ in any context, and $p(f)$ is the probability for the context $f$ across all words.\n",
    "\n",
    "If we have access to the counts for each of these factors individually:\n",
    "\n",
    "* Sum of all contexts (`matrix.total_sum`)\n",
    "* All contexts for a given word (`matrix.get_row_sum(word)`)\n",
    "* All contexts a given word appears in (`matrix.get_col_sum(word)`)\n",
    "\n",
    "Write a function that calculates PPMI for word $w$ and context word $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate PPMI for two words, using the\n",
    "# colocation matrix calculated above.\n",
    "\n",
    "import math\n",
    "\n",
    "def calculate_ppmi(w, f):\n",
    "    sum_all_context = matrix.total_sum\n",
    "    word_count = matrix.get_row_sum(w)\n",
    "    context_count = matrix.get_col_sum(f)\n",
    "    joint_count = matrix.get_pair(w,f)\n",
    "    \n",
    "    p_w = word_count/ float(sum_all_context)\n",
    "    p_f = context_count/float(sum_all_context)\n",
    "    p_w_f = joint_count/float(sum_all_context)\n",
    "    \n",
    "    return math.log2((p_w_f)/(p_w * p_f))\n",
    "\n",
    "#change tor eturn only positive#watch video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.497352638218837"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_ppmi('grand', 'jury')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_freq(w, f):   \n",
    "    return matrix.get_pair(w,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'temperature', 'in', 'the', \"O'Reilly\", 'Arbuthnot-Smythe', 'server', \"'s\", 'main', 'rack', 'is', '40.5', 'degrees']\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = \"The temperature in the O'Reilly & Arbuthnot-Smythe server's main rack is 40.5 degrees.\"\n",
    "example = word_tokenize(example)\n",
    "sent = [w for w in example if w.lower() not in string.punctuation]\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nteract": {
   "version": "0.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
