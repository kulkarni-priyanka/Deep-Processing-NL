{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_test_filename = \"../data/wsd_contexts.txt\"\n",
    "brown_corpus = brown.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "senses_list = []\n",
    "allWords ={}\n",
    "with open(wsd_test_filename, 'r') as wsd_file:\n",
    "    for line in wsd_file:\n",
    "        line = line.strip('\\n')\n",
    "        line = line.split('\\t')\n",
    "        probe_word = line[0]\n",
    "        noun_groups = line[1].split(',')\n",
    "\n",
    "        for ws in wn.synsets(probe_word, pos = wn.NOUN):\n",
    "            senses_list.append(ws)\n",
    "            for context in noun_groups:\n",
    "                for cs in wn.synsets(context, pos = wn.NOUN):\n",
    "                    senses_list.append(cs)\n",
    "                    subsumers = ws.common_hypernyms(cs)\n",
    "                    for subsumer in subsumers:\n",
    "                        if subsumer._pos == 'n':\n",
    "                            senses_list.append(subsumer)\n",
    "\n",
    "        for sense in senses_list:\n",
    "            lemma, pos, index = sense._name.split('.')\n",
    "            allWords[lemma] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aw in allWords:\n",
    "    allWords[aw] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags ={}\n",
    "for sent in brown_corpus:\n",
    "    for w,t in sent:\n",
    "        if t in tags:\n",
    "            tags[t]+=1\n",
    "        else:\n",
    "            tags[t] = 1\n",
    "        if w in allWords and t =='NN':\n",
    "            allWords[w] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum(allWords.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "with open('custom_ic.dat', 'w') as newIcFile:\n",
    "\n",
    "    # not quite sure what this does\n",
    "\n",
    "    newIcFile.write('wnver::eOS9lXC6GvMWznF1wkZofDdtbBU' + '\\n')\n",
    "    newIcFile.write('1740n ' + str(total_words) + ' ROOT\\n')\n",
    "    \n",
    "    for word in allWords:\n",
    "\n",
    "        synsets = wn.synsets(word)\n",
    "        count = allWords[word]\n",
    "        total = total_words\n",
    "        logProb = -math.log(float(count) / total)\n",
    "        \n",
    "        for synset in synsets:\n",
    "            line = str(synset._offset) + 'n' + ' ' + str(logProb) + '\\n'            \n",
    "            newIcFile.write(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tags.txt','w') as t_file:\n",
    "    for t in tags.items():\n",
    "        t_file.write(t[0]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
